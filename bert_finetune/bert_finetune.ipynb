{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f990f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, DataCollatorWithPadding, Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbdc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-german-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb0ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c982216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094c0a9eb37d40eab430ad42180aada5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bc2616b6e047fc9d362a6cce7ebb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\": \"articlesTrain_10kGNAD_10perSegment.csv\", \"test\": \"articlesTest_10kGNAD_10perSegment.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "def tokenize_function(set):\n",
    "    return tokenizer(set[\"Article\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "labels = [ \"Etat\", \"Inland\", \"International\", \"Kultur\", \"Panorama\", \"Sport\", \"Web\", \"Wirtschaft\", \"Wissenschaft\", ]\n",
    "\n",
    "def label_mapping(x):\n",
    "    return labels.index(x)\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"label\": label_mapping(x[\"Segment\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f28294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a BERT model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "model.config.id2label = {i: label for i, label in enumerate(labels)}\n",
    "model.config.label2id = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570d0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee07140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=5e-5,             \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    use_mps_device=True,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91ad57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3d8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5a68dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # Pre-trained BERT model\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,        # Efficient batching\n",
    "    compute_metrics=compute_metrics     # Custom metric function\n",
    ")\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27da5a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 49/140 00:28 < 00:54, 1.67 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.978640</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.753952</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.536484</td>\n",
       "      <td>0.585859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.378393</td>\n",
       "      <td>0.646465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.250690</td>\n",
       "      <td>0.626263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.266021</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.303836</td>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49, training_loss=0.8562723276566486, metrics={'train_runtime': 32.4521, 'train_samples_per_second': 61.013, 'train_steps_per_second': 4.314, 'total_flos': 45586855302912.0, 'train_loss': 0.8562723276566486, 'epoch': 7.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c076d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2506896257400513,\n",
       " 'eval_accuracy': 0.6262626262626263,\n",
       " 'eval_runtime': 0.6268,\n",
       " 'eval_samples_per_second': 157.939,\n",
       " 'eval_steps_per_second': 11.167,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b409577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Wirtschaft (index: 7)\n"
     ]
    }
   ],
   "source": [
    "D = dataset[\"test\"].select([10]).remove_columns([\"Segment\", \"Article\"])\n",
    "predictions = trainer.predict(D)\n",
    "predicted_class = predictions.predictions.argmax(axis=-1)[0]\n",
    "print(f\"Predicted class: {labels[predicted_class]} (index: {predicted_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a9c2519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline using the trained model\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=False,\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Function to classify text(s) and return label names\n",
    "def classify_text(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    results = classifier(texts)\n",
    "    return [result['label'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb643fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Inland', 'score': 0.627457857131958}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"Der angekündigte Umbau des Landtagsklubs schreitet zäh voran, in der Stadt ist kein logischer Nachfolger für Bürgermeister Schaden in Sicht. Er hat ein bisserl warten müssen, aber im Dezember dürfte er den Sprung in den Landtag schaffen: Tarik Mete, 28 Jahre alt und eine der wenigen echten Nachwuchshoffnungen der Salzburger SPÖ. Der rote Jungstar mit türkischen Wurzeln, der bei der Landtagswahl 2013 immerhin 1.800 Vorzugsstimmen erreichte, soll das Mandat von Nicole Solarz (34) übernehmen, die in Karenz gehen wird. Derzeit ist Mete als Assistent des Obmanns bei der Salzburger Gebietskrankenkasse beschäftigt. Landesparteiobmann und Landtagsklubobmann Walter Steidl bestätigt im STANDARD-Gespräch entsprechende Pläne. Damit sei der zweite personalpolitische Parteivorstandsbeschluss umgesetzt, sagt Steidl. Dieser habe Metes Karriere betroffen, der andere den Bezirk Lungau. Nach dem aus privaten Gründen erfolgten Rückzug des Schwarzacher Bürgermeisters Andreas Haitzer aus dem Landtag ist der 1972 geborene Bürgermeister von St. Margarethen, Gerd Brand, nachgerückt. Der von Steidl bald nach der Wahl 2013 angekündigte Umbau des Landtagsklubs schreitet mit dem Nachrücken Metes nun zwar voran, allerdings zäh. Die lange Zeit als Ablösekandidatin gehandelte zweite Landtagspräsidentin Gudrun Mosler-Törnström (59) wird wohl bis zur nächsten Wahl bleiben. Offen ist, ob die Landesgeschäftsführerin des ÖGB, Heidi Hirschbichler (56), sich früher aus dem Landtag zurückzieht. Mittelfristig ist für die Sozialdemokraten an der Salzach freilich die Frage wesentlich bedeutsamer, wer Langzeitbürgermeister Heinz Schaden nachfolgen soll. Schaden ist seit 1992 Mitglied der Stadtregierung und seit 1999 Bürgermeister. Der 61-Jährige hat wiederholt angekündigt, bei der Bürgermeister- und Gemeinderatswahl im März 2019 nicht mehr anzutreten. Eine Entscheidung über seine Nachfolge als SPÖ-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5a4208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved to 'bert_classifier_pipeline.jl'\n",
      "Pipeline saved to 'bert_classifier pipeline'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the classifier pipeline to a file\n",
    "with open('bert_classifier_pipeline.jl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "print(\"Pipeline saved to 'bert_classifier_pipeline.jl'\")\n",
    "\n",
    "classifier.save_pretrained(\"bert_classifier_pipeline\")\n",
    "print(\"Pipeline saved to 'bert_classifier pipeline'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea39c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
