{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "e8d25dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1243ccb10>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, FloatTensor, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "import bank_dataset\n",
    "import importlib\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "importlib.reload(bank_dataset)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "718da4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_dataset = bank_dataset.BankDataset(\"./bank_dataset.csv\")\n",
    "bank_dataset_train, bank_dataset_test = random_split(bank_dataset, [0.7, 0.3])\n",
    "bank_dataloader_train = DataLoader(bank_dataset_train, batch_size=1, shuffle=True)\n",
    "bank_dataloader_test = DataLoader(bank_dataset_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "8976e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_size: int,  head_size: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.k = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.v = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, X: Tensor): \n",
    "        keys = self.k(X)\n",
    "        queries = self.q(X)\n",
    "        values = self.v(X)\n",
    "\n",
    "        scores = queries.matmul(keys.T)\n",
    "        scores = scores / torch.sqrt(torch.tensor(self.head_size))\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        res = scores.matmul(values)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "f8c47fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % n_heads == 0\n",
    "\n",
    "        head_size = embedding_dim // n_heads\n",
    "        self.ath = nn.ModuleList([AttentionHead(embedding_dim, head_size) for _ in range(n_heads)])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = [ath(X) for ath in self.ath]\n",
    "        X = torch.cat(X, dim=1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "72cf4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, inner: nn.Module):\n",
    "        super().__init__()\n",
    "        self.inner = inner\n",
    "\n",
    "    def forward(self, X):\n",
    "        r = self.inner(X)\n",
    "        return r+X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "3c953968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, context_size, embedding_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            ResidualAdd(MultiHeadAttention(embedding_dim, n_heads)),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embedding_dim, embedding_dim * 3),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(embedding_dim * 3, embedding_dim),\n",
    "                )\n",
    "            ),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "3d1f217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "8d36458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProMax(nn.Module):\n",
    "    def __init__(self, context_size, n_embeddings, embedding_dim, transformer_count, n_heads, n_classes):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, context_size, 0.1)\n",
    "        self.layers = nn.Sequential(\n",
    "            *(Transformer(context_size, embedding_dim, n_heads) for _ in range(transformer_count)),\n",
    "            nn.Linear(embedding_dim, n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.embeddings(X) \n",
    "        # X += self.positional_encoding(torch.arange(0, self.context_size))\n",
    "        X = self.positional_encoding(X)\n",
    "        X = self.layers(X)\n",
    "        X = X.mean(dim=0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "7269498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num parameters: 14'"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertProMax(\n",
    "    context_size=bank_dataset.context_window_size,\n",
    "    n_embeddings=bank_dataset.unique_word_count,\n",
    "    embedding_dim=4,\n",
    "    transformer_count=1,\n",
    "    n_heads=1,\n",
    "    n_classes=3,\n",
    ")\n",
    "f\"Num parameters: {len(list(model.parameters()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "efdc2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "d58a2d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d28999dbb384bf5a0496c43764156ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1.131457233239734\n",
      "Epoch 2: 1.1008698675367568\n",
      "Epoch 3: 1.0714089069416914\n",
      "Epoch 4: 0.9306960435456069\n",
      "Epoch 5: 0.7637306719072281\n",
      "Epoch 6: 0.6266702467960025\n",
      "Epoch 7: 0.5917177373770053\n",
      "Epoch 8: 0.5971649657323878\n",
      "Epoch 9: 0.5423659943832607\n",
      "Epoch 10: 0.5082604631820999\n",
      "Epoch 11: 0.43663948488535076\n",
      "Epoch 12: 0.4880524722701579\n",
      "Epoch 13: 0.35839014917257284\n",
      "Epoch 14: 0.4623039522745385\n",
      "Epoch 15: 0.3249903495605818\n",
      "Epoch 16: 0.3417173612951523\n",
      "Epoch 17: 0.35367207175426224\n",
      "Epoch 18: 0.3063786306157314\n",
      "Epoch 19: 0.29107093656800254\n",
      "Epoch 20: 0.1911649764564736\n",
      "Epoch 21: 0.23931706377191755\n",
      "Epoch 22: 0.2590085105876877\n",
      "Epoch 23: 0.15993791033154126\n",
      "Epoch 24: 0.13320601594550585\n",
      "Epoch 25: 0.15863483605620565\n",
      "Epoch 26: 0.14448339625128678\n",
      "Epoch 27: 0.18645128893429444\n",
      "Epoch 28: 0.09056400069129214\n",
      "Epoch 29: 0.07143258584278916\n",
      "Epoch 30: 0.08710216058827148\n",
      "Epoch 31: 0.13949890564708287\n",
      "Epoch 32: 0.11339520558680373\n",
      "Epoch 33: 0.19753577679681478\n",
      "Epoch 34: 0.10590118887186267\n",
      "Epoch 35: 0.12317401238855112\n",
      "Epoch 36: 0.11558838093606509\n",
      "Epoch 37: 0.11353792134357035\n",
      "Epoch 38: 0.07913126804614587\n",
      "Epoch 39: 0.08770662755027867\n",
      "Epoch 40: 0.10480987535797\n",
      "Epoch 41: 0.07553194847207999\n",
      "Epoch 42: 0.11023870207031784\n",
      "Epoch 43: 0.06688790103277714\n",
      "Epoch 44: 0.038421913718425287\n",
      "Epoch 45: 0.09777302250528472\n",
      "Epoch 46: 0.06663664106388019\n",
      "Epoch 47: 0.0609302793019057\n",
      "Epoch 48: 0.05766648481410273\n",
      "Epoch 49: 0.07024187640386767\n",
      "Epoch 50: 0.045512200723076976\n",
      "Epoch 51: 0.057176642227676454\n",
      "Epoch 52: 0.047362401713149024\n",
      "Epoch 53: 0.045144470305617516\n",
      "Epoch 54: 0.044959634464701005\n",
      "Epoch 55: 0.03202393576347272\n",
      "Epoch 56: 0.07864292552888703\n",
      "Epoch 57: 0.025561387389581084\n",
      "Epoch 58: 0.03789531875176296\n",
      "Epoch 59: 0.015946639784569876\n",
      "Epoch 60: 0.0412873022723943\n",
      "Epoch 61: 0.049393524705135465\n",
      "Epoch 62: 0.05343713414212861\n",
      "Epoch 63: 0.060031099104976636\n",
      "Epoch 64: 0.03006743114179391\n",
      "Epoch 65: 0.028783144004163753\n",
      "Epoch 66: 0.03161040161300599\n",
      "Epoch 67: 0.030707853160483335\n",
      "Epoch 68: 0.040347107228025036\n",
      "Epoch 69: 0.004517721194807328\n",
      "Epoch 70: 0.007862752994138867\n",
      "Epoch 71: 0.00976189805028274\n",
      "Epoch 72: 0.015844633968366974\n",
      "Epoch 73: 0.009236765834988324\n",
      "Epoch 74: 0.028586320395180054\n",
      "Epoch 75: 0.031129692577817072\n",
      "Epoch 76: 0.02252938788003347\n",
      "Epoch 77: 0.004079188097656933\n",
      "Epoch 78: 0.009401484415960752\n",
      "Epoch 79: 0.00886504553877857\n",
      "Epoch 80: 0.005085429185970815\n",
      "Epoch 81: 0.003369740943184605\n",
      "Epoch 82: 0.01154976375293998\n",
      "Epoch 83: 0.0211588828050596\n",
      "Epoch 84: 0.007910536664153537\n",
      "Epoch 85: 0.0027792309769230235\n",
      "Epoch 86: 0.007195740346351014\n",
      "Epoch 87: 0.004429404352476297\n",
      "Epoch 88: 0.004999171277949432\n",
      "Epoch 89: 0.0026924934507564654\n",
      "Epoch 90: 0.0020178399747956977\n",
      "Epoch 91: 0.002719379847989061\n",
      "Epoch 92: 0.0011829820156250477\n",
      "Epoch 93: 0.015430721127715092\n",
      "Epoch 94: 0.0044971385091747635\n",
      "Epoch 95: 0.008250088844867061\n",
      "Epoch 96: 0.0025482447902976236\n",
      "Epoch 97: 0.005348286745835313\n",
      "Epoch 98: 0.0026785174115259726\n",
      "Epoch 99: 0.00957011373049631\n",
      "Epoch 100: 0.022153900367840602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertProMax(\n",
       "  (embeddings): Embedding(651, 4)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Transformer(\n",
       "      (layers): Sequential(\n",
       "        (0): ResidualAdd(\n",
       "          (inner): MultiHeadAttention(\n",
       "            (ath): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q): Linear(in_features=4, out_features=4, bias=False)\n",
       "                (k): Linear(in_features=4, out_features=4, bias=False)\n",
       "                (v): Linear(in_features=4, out_features=4, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ResidualAdd(\n",
       "          (inner): Sequential(\n",
       "            (0): Linear(in_features=4, out_features=12, bias=True)\n",
       "            (1): SiLU()\n",
       "            (2): Linear(in_features=12, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=4, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    loss_total = 0\n",
    "    for x, y in bank_dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x[0])\n",
    "        loss = loss_fn(y_pred, y[0])\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {i+1}: {loss_total / len(bank_dataloader_train)}\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d36babd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval(index: int):\n",
    "    x, y = bank_dataset.__getitem__(index)\n",
    "    y_pred = model(x)\n",
    "    s = F.softmax(y_pred, dim=-1) \n",
    "    highest_class = torch.argmax(s)\n",
    "    print(bank_dataset.df[\"Satz\"][index])\n",
    "    print(f\"Predicted: {bank_dataset.index2label(highest_class.item())} ({s[highest_class].item()*100}%)\")\n",
    "    print(f\"True:      {bank_dataset.index2label(y.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "e04a398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def loss_eval(index: int):\n",
    "    x, y = bank_dataset.__getitem__(index)\n",
    "    y_pred = model(x)\n",
    "    return loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "43a763ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = torch.tensor([loss_eval(index) for index in range(len(bank_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "cc746623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In der Bank herrschte Hochbetrieb, weil viele ihre Steuerbescheide einzahlen wollten.\n",
      "Predicted: Sitzbank (99.48650002479553%)\n",
      "True:      Geldbank\n"
     ]
    }
   ],
   "source": [
    "eval(all_losses.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "9810c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1953512728214264\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_val_loss = 0\n",
    "    for x, y in bank_dataloader_test:\n",
    "        y_pred = model(x[0])\n",
    "        loss = loss_fn(y_pred, y[0])\n",
    "        total_val_loss += loss\n",
    "    avg_val_loss = total_val_loss / len(bank_dataloader_test)\n",
    "\n",
    "print(f\"Val Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
