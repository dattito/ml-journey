{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "e8d25dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, FloatTensor, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "import bank_dataset\n",
    "import importlib\n",
    "from tqdm.notebook import tqdm\n",
    "from torchinfo import summary\n",
    "import math\n",
    "\n",
    "importlib.reload(bank_dataset)\n",
    "\n",
    "torch.manual_seed(43);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "718da4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_dataset = bank_dataset.BankDataset(\"./bank_dataset.csv\")\n",
    "bank_dataset_train, bank_dataset_test = random_split(bank_dataset, [0.8, 0.2])\n",
    "bank_dataloader_train = DataLoader(bank_dataset_train, batch_size=4, shuffle=True)\n",
    "bank_dataloader_test = DataLoader(bank_dataset_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_size: int, head_size: int, causal_masking=True):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.k = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.v = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "        self.causal_masking = causal_masking\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        b, c, e = X.shape\n",
    "        keys: Tensor = self.k(X)\n",
    "        queries: Tensor = self.q(X)\n",
    "        values = self.v(X)\n",
    "\n",
    "        scores = queries.matmul(keys.transpose(-2, -1))\n",
    "        scores = scores / torch.sqrt(torch.tensor(self.head_size))\n",
    "\n",
    "        if self.causal_masking:\n",
    "            mask = torch.tril(torch.ones(c, c))\n",
    "            mask = mask.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            scores += mask.unsqueeze(0)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        res = scores.matmul(values)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "f8c47fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % n_heads == 0\n",
    "\n",
    "        head_size = embedding_dim // n_heads\n",
    "        self.ath = nn.ModuleList([AttentionHead(embedding_dim, head_size) for _ in range(n_heads)])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = [ath(X) for ath in self.ath]\n",
    "        X = torch.cat(X, dim=-1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "72cf4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, inner: nn.Module):\n",
    "        super().__init__()\n",
    "        self.inner = inner\n",
    "\n",
    "    def forward(self, X):\n",
    "        r = self.inner(X)\n",
    "        return r+X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "3c953968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            ResidualAdd(MultiHeadAttention(embedding_dim, n_heads)),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embedding_dim, embedding_dim * 3),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(embedding_dim * 3, embedding_dim),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "3d1f217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        seq_len = x.size(1)  # Changed from x.size(0) to x.size(1)\n",
    "        x = x + self.pe[:seq_len, :].unsqueeze(0)  # Add batch dimension\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "8d36458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProMax(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, transformer_count, n_heads, n_classes):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            *(Transformer(embedding_dim, n_heads) for _ in range(transformer_count)),\n",
    "            nn.Linear(embedding_dim, n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.embeddings(X) \n",
    "        X = self.positional_encoding(X)\n",
    "        X = self.layers(X)\n",
    "        X = X.mean(dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "7269498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertProMax                                              --\n",
       "├─Embedding: 1-1                                        2,604\n",
       "├─PositionalEncoding: 1-2                               --\n",
       "│    └─Dropout: 2-1                                     --\n",
       "├─Sequential: 1-3                                       --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─Sequential: 3-1                             176\n",
       "│    └─Linear: 2-3                                      15\n",
       "================================================================================\n",
       "Total params: 2,795\n",
       "Trainable params: 2,795\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertProMax(\n",
    "    n_embeddings=bank_dataset.unique_word_count,\n",
    "    embedding_dim=4,\n",
    "    transformer_count=1,\n",
    "    n_heads=2,\n",
    "    n_classes=3,\n",
    ")\n",
    "\n",
    "summary(model, input=(26,), dtypes=[torch.int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "efdc2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "d58a2d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3983ba71f26e4b358501a5cf7280ac73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1.1692737716215629\n",
      "Epoch 2: 1.1405173065485779\n",
      "Epoch 3: 1.1227147181828816\n",
      "Epoch 4: 1.1160998256118209\n",
      "Epoch 5: 1.1036120178522888\n",
      "Epoch 6: 1.0951361832795319\n",
      "Epoch 7: 1.094039919199767\n",
      "Epoch 8: 1.085152174587603\n",
      "Epoch 9: 1.0774236343525074\n",
      "Epoch 10: 1.066128549752412\n",
      "Epoch 11: 1.0393584812128986\n",
      "Epoch 12: 1.0183351702160306\n",
      "Epoch 13: 0.9670636102005288\n",
      "Epoch 14: 0.9318198374024144\n",
      "Epoch 15: 0.8665904794578199\n",
      "Epoch 16: 0.8039727669071268\n",
      "Epoch 17: 0.7458708480828338\n",
      "Epoch 18: 0.7052695546989087\n",
      "Epoch 19: 0.6857538648225643\n",
      "Epoch 20: 0.6781817903673207\n",
      "Epoch 21: 0.6434855926092025\n",
      "Epoch 22: 0.6040636375546455\n",
      "Epoch 23: 0.5482117403988485\n",
      "Epoch 24: 0.5985728360160634\n",
      "Epoch 25: 0.5627950305740038\n",
      "Epoch 26: 0.5284068460265795\n",
      "Epoch 27: 0.5116041174365414\n",
      "Epoch 28: 0.5089780762791634\n",
      "Epoch 29: 0.48604126302180467\n",
      "Epoch 30: 0.49497893425049605\n",
      "Epoch 31: 0.4790869417289893\n",
      "Epoch 32: 0.45055737194639667\n",
      "Epoch 33: 0.45813870524849604\n",
      "Epoch 34: 0.43122081458568573\n",
      "Epoch 35: 0.4415843507481946\n",
      "Epoch 36: 0.4286751576388876\n",
      "Epoch 37: 0.40666827180043413\n",
      "Epoch 38: 0.3984994443340434\n",
      "Epoch 39: 0.35001325220973406\n",
      "Epoch 40: 0.3722723003011197\n",
      "Epoch 41: 0.3974136459744639\n",
      "Epoch 42: 0.3395002964470122\n",
      "Epoch 43: 0.33022678564130153\n",
      "Epoch 44: 0.33638135309296624\n",
      "Epoch 45: 0.27304928818786584\n",
      "Epoch 46: 0.29535270138229763\n",
      "Epoch 47: 0.27180858219853016\n",
      "Epoch 48: 0.31980562713687066\n",
      "Epoch 49: 0.21172178696185626\n",
      "Epoch 50: 0.2556363368855306\n",
      "Epoch 51: 0.2578277622728988\n",
      "Epoch 52: 0.25760480104428196\n",
      "Epoch 53: 0.2436173523310572\n",
      "Epoch 54: 0.2513765673222089\n",
      "Epoch 55: 0.2291518528773277\n",
      "Epoch 56: 0.18157590942940227\n",
      "Epoch 57: 0.1914471917682224\n",
      "Epoch 58: 0.19064520637254678\n",
      "Epoch 59: 0.19883644462701072\n",
      "Epoch 60: 0.1470198080599032\n",
      "Epoch 61: 0.19082262404952888\n",
      "Epoch 62: 0.16500640519101312\n",
      "Epoch 63: 0.14520972388520562\n",
      "Epoch 64: 0.15394147962366264\n",
      "Epoch 65: 0.1860760325755648\n",
      "Epoch 66: 0.10779181984253228\n",
      "Epoch 67: 0.14050325890140677\n",
      "Epoch 68: 0.1366952648989994\n",
      "Epoch 69: 0.1391975751482985\n",
      "Epoch 70: 0.13796630222781528\n",
      "Epoch 71: 0.12225292948143626\n",
      "Epoch 72: 0.12097039738566512\n",
      "Epoch 73: 0.12358715737031566\n",
      "Epoch 74: 0.111303399485122\n",
      "Epoch 75: 0.10527722370655586\n",
      "Epoch 76: 0.14324958295199192\n",
      "Epoch 77: 0.0863322750255638\n",
      "Epoch 78: 0.07894118799379578\n",
      "Epoch 79: 0.11019562758695058\n",
      "Epoch 80: 0.1123421634137803\n",
      "Epoch 81: 0.10745359206554929\n",
      "Epoch 82: 0.06918487699348824\n",
      "Epoch 83: 0.06240696732506708\n",
      "Epoch 84: 0.15487466276950995\n",
      "Epoch 85: 0.11970910053329405\n",
      "Epoch 86: 0.10996217735508387\n",
      "Epoch 87: 0.1022970979490007\n",
      "Epoch 88: 0.04821324882666684\n",
      "Epoch 89: 0.08904834631916687\n",
      "Epoch 90: 0.05237614366450106\n",
      "Epoch 91: 0.11358561895435236\n",
      "Epoch 92: 0.05563652533520427\n",
      "Epoch 93: 0.052115758693414845\n",
      "Epoch 94: 0.03893387208140835\n",
      "Epoch 95: 0.09985195294818065\n",
      "Epoch 96: 0.07935043955782291\n",
      "Epoch 97: 0.07475375453941524\n",
      "Epoch 98: 0.04892558748273317\n",
      "Epoch 99: 0.03452880145481753\n",
      "Epoch 100: 0.046666622138151\n",
      "Epoch 101: 0.03417716504083256\n",
      "Epoch 102: 0.1037518676441525\n",
      "Epoch 103: 0.07373572406747068\n",
      "Epoch 104: 0.07535722237339036\n",
      "Epoch 105: 0.07800697373588467\n",
      "Epoch 106: 0.07540230476998086\n",
      "Epoch 107: 0.06240247336056739\n",
      "Epoch 108: 0.06572357455594034\n",
      "Epoch 109: 0.05953960082735176\n",
      "Epoch 110: 0.07755858532079772\n",
      "Epoch 111: 0.09127320702235145\n",
      "Epoch 112: 0.04228431236158401\n",
      "Epoch 113: 0.12806141745467256\n",
      "Epoch 114: 0.02685266396858626\n",
      "Epoch 115: 0.056684295744398025\n",
      "Epoch 116: 0.033169579328387044\n",
      "Epoch 117: 0.0785122939543928\n",
      "Epoch 118: 0.0643679006979154\n",
      "Epoch 119: 0.0509635285890437\n",
      "Epoch 120: 0.03326469802521859\n",
      "Epoch 121: 0.037732756114564836\n",
      "Epoch 122: 0.07988861434784701\n",
      "Epoch 123: 0.06614988409166\n",
      "Epoch 124: 0.0774788982547492\n",
      "Epoch 125: 0.047281437076485924\n",
      "Epoch 126: 0.04742801697338345\n",
      "Epoch 127: 0.09074154439494359\n",
      "Epoch 128: 0.03623297044477011\n",
      "Epoch 129: 0.04953496697837383\n",
      "Epoch 130: 0.030434346857536758\n",
      "Epoch 131: 0.05053100010317629\n",
      "Epoch 132: 0.04780379651896914\n",
      "Epoch 133: 0.011924070340615732\n",
      "Epoch 134: 0.043131474957960936\n",
      "Epoch 135: 0.027500397761792153\n",
      "Epoch 136: 0.045246852582311946\n",
      "Epoch 137: 0.03797520395353247\n",
      "Epoch 138: 0.02887301017094783\n",
      "Epoch 139: 0.02210159673726324\n",
      "Epoch 140: 0.02682219747711551\n",
      "Epoch 141: 0.04776338306826936\n",
      "Epoch 142: 0.035377164252471026\n",
      "Epoch 143: 0.03084485740907473\n",
      "Epoch 144: 0.018056736761570425\n",
      "Epoch 145: 0.025480628238559195\n",
      "Epoch 146: 0.02699550623354433\n",
      "Epoch 147: 0.041103874249664066\n",
      "Epoch 148: 0.029458737363273504\n",
      "Epoch 149: 0.08956948014168085\n",
      "Epoch 150: 0.06522826002307099\n",
      "Epoch 151: 0.018350733268393756\n",
      "Epoch 152: 0.04336762732041448\n",
      "Epoch 153: 0.03263492392552844\n",
      "Epoch 154: 0.024977311556910905\n",
      "Epoch 155: 0.042979977332935154\n",
      "Epoch 156: 0.03092164662262399\n",
      "Epoch 157: 0.024345781041052693\n",
      "Epoch 158: 0.020070508702364924\n",
      "Epoch 159: 0.035072339065125024\n",
      "Epoch 160: 0.03073955996013764\n",
      "Epoch 161: 0.012836237696858545\n",
      "Epoch 162: 0.01415977592592686\n",
      "Epoch 163: 0.010080904752410587\n",
      "Epoch 164: 0.021253164834181208\n",
      "Epoch 165: 0.04438184798507589\n",
      "Epoch 166: 0.027506221985310764\n",
      "Epoch 167: 0.04704475127571776\n",
      "Epoch 168: 0.010885884098199851\n",
      "Epoch 169: 0.022925209736664936\n",
      "Epoch 170: 0.013995470961904007\n",
      "Epoch 171: 0.027930800226049206\n",
      "Epoch 172: 0.04430799045022232\n",
      "Epoch 173: 0.018930738488658488\n",
      "Epoch 174: 0.03988243887633197\n",
      "Epoch 175: 0.08961415521938491\n",
      "Epoch 176: 0.021553145292003778\n",
      "Epoch 177: 0.04278285671164297\n",
      "Epoch 178: 0.020725709830542286\n",
      "Epoch 179: 0.02733753729477973\n",
      "Epoch 180: 0.040476950477804405\n",
      "Epoch 181: 0.020229272233134706\n",
      "Epoch 182: 0.016143276826739935\n",
      "Epoch 183: 0.015970502443923142\n",
      "Epoch 184: 0.021454924785858667\n",
      "Epoch 185: 0.07480242756517125\n",
      "Epoch 186: 0.03051446261357611\n",
      "Epoch 187: 0.026064629025818756\n",
      "Epoch 188: 0.02942915836104442\n",
      "Epoch 189: 0.03511487800197857\n",
      "Epoch 190: 0.01921327246196193\n",
      "Epoch 191: 0.022205131249965808\n",
      "Epoch 192: 0.020940216932912235\n",
      "Epoch 193: 0.014288206110601701\n",
      "Epoch 194: 0.01118306094755368\n",
      "Epoch 195: 0.02513911975794751\n",
      "Epoch 196: 0.016215114598809224\n",
      "Epoch 197: 0.005155053634384617\n",
      "Epoch 198: 0.037484880441600886\n",
      "Epoch 199: 0.023439745329462178\n",
      "Epoch 200: 0.023313888426518707\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    loss_total = 0\n",
    "    for x, y in bank_dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {i+1}: {loss_total / len(bank_dataloader_train)}\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "d36babd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval(index: int):\n",
    "    x, y = bank_dataset.__getitem__(index)\n",
    "    y_pred = model(x.unsqueeze(0))\n",
    "    s = F.softmax(y_pred, dim=-1) \n",
    "    highest_class = torch.argmax(s)\n",
    "    print(bank_dataset.df[\"Satz\"][index])\n",
    "    print(f\"Predicted: {bank_dataset.index2label(highest_class.item())} ({s[0][highest_class].item()*100}%)\")\n",
    "    print(f\"True:      {bank_dataset.index2label(y.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "e04a398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def loss_eval(index: int):\n",
    "    x, y = bank_dataset.__getitem__(index)\n",
    "    y_pred = model(x.unsqueeze(0))\n",
    "    return loss_fn(y_pred, y.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "43a763ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = torch.tensor([loss_eval(index) for index in range(len(bank_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "cc746623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Bank verschickte ein Schreiben, das über neue Datenschutzrichtlinien informierte.\n",
      "Predicted: Flussbank (99.8314380645752%)\n",
      "True:      Geldbank\n"
     ]
    }
   ],
   "source": [
    "eval(all_losses.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([197, 558, 410, 118, 388, 549, 523,  20, 113, 372, 617, 193,   2,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Val Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_val_loss = 0\n",
    "    for x, y in bank_dataloader_test:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        total_val_loss += loss\n",
    "    avg_val_loss = total_val_loss / len(bank_dataloader_test)\n",
    "\n",
    "print(f\"Val Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98561e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
